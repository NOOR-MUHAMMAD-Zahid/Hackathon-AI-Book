# AI Robotics Course: From ROS 2 to Vision-Language-Action Integration

<div align="center">
  <h3>A Comprehensive Educational Resource for AI-Driven Robotics Development</h3>

  ![ROS 2](https://img.shields.io/badge/ROS2-Humble-blue?logo=ros)
  ![Docusaurus](https://img.shields.io/badge/Docusaurus-3.9.2-informational?logo=docusaurus)
  ![NVIDIA Isaac](https://img.shields.io/badge/NVIDIA-Isaac-orange?logo=nvidia)
  ![License](https://img.shields.io/badge/License-MIT-green)
</div>

## ğŸ“š Course Overview

Welcome to the comprehensive AI Robotics course that takes you from fundamental ROS 2 concepts to advanced Vision-Language-Action (VLA) integration for humanoid robotics. This course is designed for students and developers interested in building intelligent, language-driven autonomous robotic systems.

### ğŸ¯ Learning Objectives

- Master ROS 2 architecture, communication patterns, and URDF modeling
- Understand digital twin simulation with Gazebo and Unity
- Leverage NVIDIA Isaac technologies for accelerated perception and navigation
- Implement voice-to-action interfaces using OpenAI Whisper
- Develop cognitive planning systems with vision-language models
- Build complete autonomous humanoid systems

## ğŸ“– Course Modules

### Module 1: ROS 2 Robotic Nervous System
- **Introduction to ROS 2**: Architecture, nodes, topics, services, and actions
- **Communication Patterns**: Publishers, subscribers, and message passing
- **URDF Modeling**: Robot description and kinematic modeling
- **Hands-on Projects**: Create and simulate basic robot models

### Module 2: Digital Twin Simulation for Physical AI
- **Digital Twin Concepts**: Bridging physical and virtual worlds
- **Physics Simulation**: Advanced Gazebo simulation with realistic physics
- **High-Fidelity Sensors**: Accurate sensor modeling and simulation
- **Validation Techniques**: Ensuring simulation-to-reality transfer

### Module 3: AI-Robot Brain with NVIDIA Isaac
- **NVIDIA Isaac Sim**: Photorealistic simulation and synthetic data generation
- **Isaac ROS**: Hardware-accelerated perception pipelines using GPU and Tensor Core acceleration
- **Autonomous Navigation**: Nav2 stack adapted for advanced robotics applications
- **Real-world Integration**: From simulation to physical robot deployment

### Module 4: Vision-Language-Action Integration for Humanoid Robotics
- **Voice-to-Action Interfaces**: OpenAI Whisper for speech-to-text processing
- **Cognitive Planning**: LLM-based task decomposition and reasoning
- **Perception-Grounded Planning**: Visual feedback loops for adaptive behavior
- **Autonomous Humanoid Capstone**: Complete system integration

## ğŸš€ Getting Started

### Prerequisites
- Basic programming knowledge (Python, C++)
- Understanding of robotics concepts
- Linux environment (Ubuntu 22.04 recommended for ROS 2)
- Node.js >= 20.0 for documentation

### Local Development

1. **Clone the repository**:
   ```bash
   git clone https://github.com/NOOR-MUHAMMAD-Zahid/Hackathon-AI-Book.git
   cd Hackathon-AI-Book
   ```

2. **Navigate to the frontend book directory**:
   ```bash
   cd frontend_book
   ```

3. **Install dependencies**:
   ```bash
   npm install
   ```

4. **Start the development server**:
   ```bash
   npm start
   ```

5. **Open your browser** to [http://localhost:3000](http://localhost:3000)

### Documentation Structure

The course documentation is built with Docusaurus and organized as follows:

```
frontend_book/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ intro.md
â”‚   â”œâ”€â”€ glossary.md
â”‚   â”œâ”€â”€ ros2-nervous-system/
â”‚   â”œâ”€â”€ digital-twin-simulation/
â”‚   â”œâ”€â”€ ai-robot-brain-isaac/
â”‚   â””â”€â”€ vla-integration/
â”œâ”€â”€ src/
â”œâ”€â”€ static/
â”œâ”€â”€ docusaurus.config.js
â””â”€â”€ sidebars.js
```

## ğŸ¤– Technologies & Tools

- **ROS 2 (Humble Hawksbill)**: Robot Operating System for communication and control
- **Docusaurus**: Static site generator for documentation
- **NVIDIA Isaac Sim**: Photorealistic simulation environment
- **Isaac ROS**: Hardware-accelerated perception packages
- **Nav2**: Navigation stack for autonomous navigation
- **OpenAI Whisper**: Speech recognition for voice interfaces
- **Gazebo**: Physics simulation environment
- **Unity**: 3D visualization and simulation (optional)

## ğŸ“š Additional Resources

- [ROS 2 Documentation](https://docs.ros.org/en/rolling/)
- [NVIDIA Isaac Documentation](https://nvidia-isaac.readthedocs.io/)
- [Docusaurus Documentation](https://docusaurus.io/)
- [Nav2 Documentation](https://navigation.ros.org/)

## ğŸ¤ Contributing

We welcome contributions to improve this educational resource! Please feel free to:
- Submit pull requests for corrections or improvements
- Open issues for bugs or feature requests
- Share your own robotics projects and examples

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¥ Authors & Maintainers

This course was developed as part of a hackathon project to create an educational resource for AI-driven robotics development.

## ğŸŒŸ Acknowledgments

- The ROS 2 community for their excellent framework
- NVIDIA for the Isaac platform and tools
- The Docusaurus team for the documentation platform
- All contributors to open-source robotics projects that made this possible

---

<div align="center">
  <h3>Ready to start your journey in AI robotics? Begin with <a href="https://github.com/NOOR-MUHAMMAD-Zahid/Hackathon-AI-Book/tree/main/frontend_book/docs/intro.md">the introduction</a> and explore the modules!</h3>
  <p>Happy coding and building! ğŸ¤–</p>
</div>